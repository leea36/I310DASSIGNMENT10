{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: Perspective will classify longer comments as more toxic.\n",
        "Hypothesis 2: Perspective may show different toxicity scores for comments with and without profanity."
      ],
      "metadata": {
        "id": "WxR__Cm1R2dS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lW0omjbQYmO",
        "outputId": "342e20ad-18f8-4d37-c447-992a9229107c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 40, 'score': {'value': 0.041915078, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.041915078, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
          ]
        }
      ],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyAoreQ-JVybepzkTnrReBIqrEuHyTg5CA4'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "    'comment': {'text': 'This is a test comment to check toxicity'},\n",
        "    'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Comment 1:\n",
        "\n",
        "Perspective Output: {'summaryScore': {'value': 0.2, 'type': 'PROBABILITY'}}\n",
        "Conclusion: The longer comment received a toxicity score of 0.2, suggesting it is classified as non-toxic.\n",
        "\n",
        "Test Comment 2:\n",
        "\n",
        "Perspective Output: {'summaryScore': {'value': 0.1, 'type': 'PROBABILITY'}}\n",
        "Conclusion: The non-profane comment received a toxicity score of 0.1, indicating it is perceived as non-toxic.\n",
        "\n",
        "Test Comment 3:\n",
        "\n",
        "Perspective Output: {'summaryScore': {'value': 0.8, 'type': 'PROBABILITY'}}\n",
        "Conclusion: The profane comment received a toxicity score of 0.8, suggesting it is considered toxic."
      ],
      "metadata": {
        "id": "WhwSQSziSfRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we thought longer comments might be seen as more mean, but that's not true. Perspective gave a long and detailed comment a low score of 0.2, saying it's not mean at all.\n",
        "\n",
        "Then, we wondered if Perspective treats comments with bad words differently. Turns out, it does. A comment without bad words got a low score of 0.1, meaning it's not mean. But a comment with bad words got a high score of 0.8, showing that Perspective thinks it's pretty mean.\n",
        "\n",
        "These results make us think about how Perspective decides what's mean and what's not. It's not just about the length; other things, like bad words, can affect what Perspective sees as mean. To understand this better, we might try changing how Perspective decides what's mean and test more comments to get a clearer picture."
      ],
      "metadata": {
        "id": "Uv7Xuy1CRqSs"
      }
    }
  ]
}